{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11213099,"sourceType":"datasetVersion","datasetId":7001791},{"sourceId":11214341,"sourceType":"datasetVersion","datasetId":7002708},{"sourceId":11214591,"sourceType":"datasetVersion","datasetId":7002892}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:27.298216Z","iopub.execute_input":"2025-04-23T08:58:27.298458Z","iopub.status.idle":"2025-04-23T08:58:27.636121Z","shell.execute_reply.started":"2025-04-23T08:58:27.298438Z","shell.execute_reply":"2025-04-23T08:58:27.635369Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/converted/b6_train_data_converted.csv\n/kaggle/input/fpt-ai/dataset/b6_test_data.csv\n/kaggle/input/fpt-ai/dataset/b6_train_data.csv\n/kaggle/input/fpt-ai/dataset/test_data.pkl\n/kaggle/input/fpt-ai/dataset/train_data.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Gen Reasoning","metadata":{}},{"cell_type":"code","source":"# Example of how to use this for a single question\nimport pandas as pd\n\n# Read the questions\nquestions_df = pd.read_csv(\"/kaggle/input/fpt-ai/dataset/b6_test_data.csv\")\n\n# Function to convert A/B/C... to index\ndef char_to_index(char):\n    return ord(char) - ord('A')\n\ndef format_prompt(question, answer):\n    return f\"\"\"<|im_start|>system\nGiven a question and its correct answer. Explain why this is the correct answer. Be concise and clear in your explanation.\n<|im_end|>\n<|im_start|>user\nExplain why this is the correct answer:\n{question}\nCorrect Answer:\n{answer}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:27.637585Z","iopub.execute_input":"2025-04-23T08:58:27.637978Z","iopub.status.idle":"2025-04-23T08:58:27.659776Z","shell.execute_reply.started":"2025-04-23T08:58:27.637956Z","shell.execute_reply":"2025-04-23T08:58:27.658810Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\n# Function to extract character (A, B, C...) from answer field\ndef extract_answer_char(answer):\n    if pd.isna(answer) or not isinstance(answer, str):\n        return None\n    if \"ANSWER:\" in answer:\n        answer_char = answer.split(\":\")[-1].strip()\n    else:\n        answer_char = answer.strip()\n    return answer_char\n\ndef gen_reasoning(df, batch_size=8, max_new_tokens=256):\n    # Collect processed rows\n    reasonings = {}\n\n    # Use a batch processing approach\n    num_batches = len(df) // batch_size + (1 if len(df) % batch_size > 0 else 0)\n\n    for batch_idx in tqdm(range(num_batches), total=num_batches):\n        batch_start = batch_idx * batch_size\n        batch_end = min((batch_idx + 1) * batch_size, len(df))\n        batch_df = df.iloc[batch_start:batch_end]\n\n        # Prepare the inputs for the batch\n        batch_prompts = []\n        task_ids = []\n\n        for _, row in batch_df.iterrows():\n            task_id = row['task_id']\n            question = row['question']\n            choices_str = row['choices']\n            answer_raw = row['answer']\n\n            # Extract clean answer char\n            answer_char = extract_answer_char(answer_raw)\n            if not answer_char:\n                continue  # Skip rows with no valid answer\n\n            try:\n                choices = eval(choices_str)\n            except Exception:\n                continue  # Skip rows where choices can't be evaluated\n\n            answer_index = char_to_index(answer_char)\n\n            # Get correct answer text\n            if 0 <= answer_index < len(choices):\n                correct_answer_text = choices[answer_index]\n\n            prompt = format_prompt(question, correct_answer_text)\n            batch_prompts.append(prompt)\n            task_ids.append(task_id)\n\n        # Tokenize the entire batch and pass to the model\n        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n\n        # Generate outputs for the entire batch in one go\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            top_p=1.0,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        # Decode the outputs for the entire batch\n        for task_id, output in zip(task_ids, outputs):\n            decoded = tokenizer.decode(output, skip_special_tokens=True)\n            if \"<|im_start|>assistant\" in decoded:\n                reasoning = decoded.split(\"<|im_start|>assistant\")[-1].strip()\n            else:\n                reasoning = decoded.strip()\n\n            # Save each task_id with the corresponding reasoning\n            reasonings[task_id] = reasoning\n\n            # Clear cache and free memory after each batch\n        del inputs  # Delete the input batch\n        del outputs  # Delete the model's output batch\n\n        torch.cuda.empty_cache()  # Clear the GPU memory cache\n\n    return reasonings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:27.660778Z","iopub.execute_input":"2025-04-23T08:58:27.661106Z","iopub.status.idle":"2025-04-23T08:58:29.418478Z","shell.execute_reply.started":"2025-04-23T08:58:27.661057Z","shell.execute_reply":"2025-04-23T08:58:29.417509Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/fpt-ai/dataset/b6_train_data.csv\")\n# reasonings = gen_reasoning(df,8,256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:29.419338Z","iopub.execute_input":"2025-04-23T08:58:29.419714Z","iopub.status.idle":"2025-04-23T08:58:29.423749Z","shell.execute_reply.started":"2025-04-23T08:58:29.419692Z","shell.execute_reply":"2025-04-23T08:58:29.422440Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Fine-tuning LLM","metadata":{}},{"cell_type":"code","source":"# Đọc dữ liệu\ntrain_data = pd.read_csv('/kaggle/input/fpt-ai/dataset/b6_train_data.csv')\ntest_data = pd.read_csv('/kaggle/input/fpt-ai/dataset/b6_test_data.csv')\n\nprint(\"Kích thước dữ liệu huấn luyện:\", train_data.shape)\nprint(\"Kích thước dữ liệu kiểm tra:\", test_data.shape)\n\n# Kiểm tra các giá trị null\nprint(\"\\nSố lượng giá trị null trong tập huấn luyện:\")\nprint(train_data.isnull().sum())\nprint(\"\\nSố lượng giá trị null trong tập kiểm tra:\")\nprint(test_data.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:29.424958Z","iopub.execute_input":"2025-04-23T08:58:29.425279Z","iopub.status.idle":"2025-04-23T08:58:29.497632Z","shell.execute_reply.started":"2025-04-23T08:58:29.425248Z","shell.execute_reply":"2025-04-23T08:58:29.496953Z"}},"outputs":[{"name":"stdout","text":"Kích thước dữ liệu huấn luyện: (3963, 4)\nKích thước dữ liệu kiểm tra: (1253, 3)\n\nSố lượng giá trị null trong tập huấn luyện:\ntask_id      0\nquestion     0\nchoices      0\nanswer      14\ndtype: int64\n\nSố lượng giá trị null trong tập kiểm tra:\ntask_id     0\nquestion    0\nchoices     0\ndtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Hàm để chuyển đổi chuỗi choices thành danh sách\ndef parse_choices(choices_str):\n    try:\n        if isinstance(choices_str, list):\n            return choices_str\n\n        # Sử dụng ast.literal_eval để chuyển đổi chuỗi thành danh sách\n        try:\n            return ast.literal_eval(choices_str)\n        except:\n            # Nếu không thể phân tích cú pháp, thực hiện xử lý thủ công\n            choices_str = choices_str.strip('[]')\n            choices = [choice.strip().strip(\"'\").strip('\"') for choice in choices_str.split(',')]\n            return choices\n    except:\n        print(f\"Lỗi xử lý choices: {choices_str}\")\n        return []\n\n# Áp dụng hàm parse_choices cho cả tập huấn luyện và kiểm tra\ntrain_data['parsed_choices'] = train_data['choices'].apply(parse_choices)\ntest_data['parsed_choices'] = test_data['choices'].apply(parse_choices)\n\n# Chuẩn hóa câu trả lời (một số có \"ANSWER: \" ở đầu)\ndef normalize_answer(answer_str):\n    if isinstance(answer_str, str):\n        if \"ANSWER: \" in answer_str:\n            return answer_str.split(\"ANSWER: \")[1]\n    return answer_str\n\ntrain_data['normalized_answer'] = train_data['answer'].apply(normalize_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:29.498375Z","iopub.execute_input":"2025-04-23T08:58:29.498589Z","iopub.status.idle":"2025-04-23T08:58:29.524524Z","shell.execute_reply.started":"2025-04-23T08:58:29.498570Z","shell.execute_reply":"2025-04-23T08:58:29.523599Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    Trainer\n)\nfrom peft import PeftModel, PeftConfig, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nimport warnings\n\n# Hàm định dạng dữ liệu cho mô hình\ndef format_example(row, include_answer=True):\n    question = row['question']\n    choices = row['parsed_choices']\n\n    # Định dạng các lựa chọn thành A, B, C, D\n    formatted_choices = '\\n'.join([f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices)])\n\n    if include_answer:\n        return f\"\"\"{question}\n\nOptions:\n{formatted_choices}\n\nAnswer: {row['normalized_answer']}\"\"\"\n    else:\n        return f\"\"\"{question}\n\nOptions:\n{formatted_choices}\n\nAnswer:\"\"\"\n\n# Tạo tập dữ liệu để huấn luyện\ntrain_data['text'] = train_data.apply(format_example, axis=1)\ntest_data['text'] = test_data.apply(format_example, axis=1, include_answer=False)\n\n# Phân chia tập huấn luyện và tập validation\ntrain_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n\nprint(f\"Số lượng mẫu trong tập huấn luyện: {len(train_df)}\")\nprint(f\"Số lượng mẫu trong tập validation: {len(val_df)}\")\n\n# Tạo dataset từ các pandas DataFrame\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_data)\n\n# Hiển thị một ví dụ từ tập huấn luyện\nprint(\"\\nVí dụ từ tập huấn luyện:\")\nprint(train_dataset[1]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:29.527248Z","iopub.execute_input":"2025-04-23T08:58:29.527542Z","iopub.status.idle":"2025-04-23T08:58:34.957608Z","shell.execute_reply.started":"2025-04-23T08:58:29.527521Z","shell.execute_reply":"2025-04-23T08:58:34.956657Z"}},"outputs":[{"name":"stdout","text":"Số lượng mẫu trong tập huấn luyện: 3170\nSố lượng mẫu trong tập validation: 793\n\nVí dụ từ tập huấn luyện:\nQuestion: Which of the following statements is true about list comprehension?\n\nOptions:\nA. It can only be used for creating lists of integers\nB. It can include an optional if clause for filtering elements\nC. It can include an optional if clause for filtering elements\nD. It cannot be nested\n\nAnswer: B\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:34.959305Z","iopub.execute_input":"2025-04-23T08:58:34.959803Z","iopub.status.idle":"2025-04-23T08:58:38.390309Z","shell.execute_reply.started":"2025-04-23T08:58:34.959780Z","shell.execute_reply":"2025-04-23T08:58:38.389058Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport ast\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    Trainer\n)\nfrom peft import PeftModel, PeftConfig, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:38.391661Z","iopub.execute_input":"2025-04-23T08:58:38.391983Z","iopub.status.idle":"2025-04-23T08:58:38.799678Z","shell.execute_reply.started":"2025-04-23T08:58:38.391944Z","shell.execute_reply":"2025-04-23T08:58:38.798985Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_id = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n\n# Cấu hình lượng tử hóa 4-bit để giảm memory footprint\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Tải tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Tải mô hình với cấu hình lượng tử hóa\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n\n# Chuẩn bị mô hình cho huấn luyện 4-bit\nmodel = prepare_model_for_kbit_training(model)\n\n# Cấu hình LoRA để fine-tuning\nlora_config = LoraConfig(\n    r=8,  # Rank\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n)\n\n# Áp dụng LoRA cho mô hình\npeft_model = get_peft_model(model, lora_config)\nprint(f\"Số lượng tham số huấn luyện: {peft_model.print_trainable_parameters()}\")\n\n# Hàm tiền xử lý dữ liệu để mã hóa\ndef preprocess_function(examples):\n    # Cắt bớt các câu có kích thước vượt quá max_length\n    max_length = 256  # Có thể điều chỉnh\n\n    inputs = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n\n    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n    return inputs\n\n# Áp dụng tiền xử lý\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\n\n# Thiết lập tham số huấn luyện\ntraining_args = TrainingArguments(\n    output_dir=\"results/code_mlu_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=1,\n    num_train_epochs=50,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    fp16=True,\n    report_to=\"none\",\n)\n\n# Khởi tạo trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n)\n\n# Tiến hành huấn luyện mô hình\nprint(\"Bắt đầu huấn luyện mô hình...\")\ntrainer.train()\n\n# Lưu mô hình đã huấn luyện\npeft_model.save_pretrained(\"results/code_mlu_model_final\")\ntokenizer.save_pretrained(\"results/code_mlu_model_final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T08:58:38.800473Z","iopub.execute_input":"2025-04-23T08:58:38.800909Z","iopub.status.idle":"2025-04-23T09:21:35.948429Z","shell.execute_reply.started":"2025-04-23T08:58:38.800886Z","shell.execute_reply":"2025-04-23T09:21:35.947184Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35ea8a0e721640e6819beb3a25898a2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e35920464d2411d99fdda4dad74d9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244334f6a96e48cf8a2b767fb71017e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da967f22e76b4f239c7110f6150de666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2c10c3d04b4d44b7903f927eb70bad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b1d6b07ee049689bd1a7b68e52ea35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240e98e345bf421084e6f4a2b626265b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7816b7ebe2af42028e1b032b805bf119"}},"metadata":{}},{"name":"stdout","text":"trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\nSố lượng tham số huấn luyện: None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3170 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18cd67635a24ad2924214a2a348f1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/793 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72547647b0124a8fbd8a26da6f8745ee"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"name":"stdout","text":"Bắt đầu huấn luyện mô hình...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='172' max='19850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  172/19850 20:36 < 39:46:12, 0.14 it/s, Epoch 0.43/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-aec72581d341>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Tiến hành huấn luyện mô hình\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bắt đầu huấn luyện mô hình...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Lưu mô hình đã huấn luyện\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m                     ):\n\u001b[1;32m   2529\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## Loading and Embedding","metadata":{}},{"cell_type":"markdown","source":"## RAG\n","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch \nfrom torch.nn import functional as F\nimport numpy as np\nimport pandas as pd\n\ndef vector_store(file_path):\n    data = pickle.load(open(file_path, \"rb\"))\n\n    task_id = list(data.keys())\n    embeddings = np.array(list(data.values()))\n    embeddings = torch.tensor(embeddings)\n\n    matrix_embeddings = torch.cat([embeddings], dim=0)\n    normalized_embeddings = F.normalize(matrix_embeddings, p=2, dim=1)\n\n    return task_id, normalized_embeddings\n\ndef top_k_cosine_similarity(A, B, k=3):\n    # Normalize A and B to unit vectors (L2 normalization)\n    A_norm = A / A.norm(dim=1, keepdim=True)\n    B_norm = B / B.norm(dim=1, keepdim=True)\n    \n    # Compute cosine similarity matrix\n    similarity = torch.mm(A_norm, B_norm.T)  # [n, m] matrix\n    \n    # Get the top-k indices and values from the similarity matrix\n    top_k_values, top_k_indices = torch.topk(similarity, k=k, dim=1, largest=True, sorted=True)\n    \n    return top_k_values, top_k_indices\n\n\ndef retrieval(query_index, csv_data_path, train_pkl_path, test_pkl_path, reasonings, k=3):\n    task_id, train_db = vector_store(train_pkl_path)\n    _, test_db = vector_store(test_pkl_path)\n\n    # print(f\"retrieving {k} most similar questions to query index {query_index}\")\n    top_k_values, top_k_indices = top_k_cosine_similarity(test_db, train_db, k=k)\n    rag_indices = top_k_indices[query_index].tolist()\n    top_k_task_id = [task_id[idx] for idx in rag_indices]\n\n    raw_data = pd.read_csv(csv_data_path)\n    qa_pairs = raw_data[raw_data[\"task_id\"].isin(top_k_task_id)][[\"question\", \"choices\", \"answer\", \"reasoning\"]]\n\n    return qa_pairs.to_dict(orient='records')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.949354Z","iopub.status.idle":"2025-04-23T09:21:35.949668Z","shell.execute_reply":"2025-04-23T09:21:35.949551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def index_to_letter(index):\n    try:\n        letters = ['A', 'B', 'C', 'D']\n        letter = letters[index]\n    except:\n        letter = 'A'\n    return letter\n\nletter = index_to_letter(2)\nprint(letter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.950681Z","iopub.status.idle":"2025-04-23T09:21:35.950995Z","shell.execute_reply":"2025-04-23T09:21:35.950850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nimport re\nimport pandas as pd\n\n# Read the questions\nquestions_df = pd.read_csv(\"/kaggle/input/fpt-ai/dataset/b6_test_data.csv\")\n\n\nquestions_df[\"choices\"] = questions_df[\"choices\"].apply(ast.literal_eval)\n\nimport re\n\ndef extract_index_from_output(text):\n    # Search for the pattern after \"<|im_start|>assistant\"\n    # match = re.search(r\"<\\|im_start\\|>assistant\\s*(?:ANSWER:\\s*)?(\\d+)\", text)\n    match = re.search(r\"assistant\\s*(?:ANSWER:\\s*)?(\\d+)\", text)\n    if match:\n        return int(match.group(1))  # Extract the number\n    else:\n        return 0  # Default to 0 if no match is found\n\ndef format_prompt(question, choices, qa_pairs):\n    formatted_choices = \"\\n\".join([f\"{i}. {c}\" for i, c in enumerate(choices)])\n    return f\"\"\"<|im_start|>system\nYou are a helpful AI assistant tasked with answering multiple-choice questions about coding.\n\nI will provide you with questions and their possible answer choices. I will also give you several examples. For each question:\n1. Respond solely with the index number of the correct option in the list, starting from 0.\n2. Do not include any explanations, text, or anything other than the letter of the correct answer.\n\nHere are some examples:\n\n1. Question: {qa_pairs[0]['question']}\n   Choices:\n   {qa_pairs[0]['choices']}\n   Answer: {qa_pairs[0]['answer']}\n   Reasoning: {qa_pairs[0]['reasoning']}\n\n2. Question: {qa_pairs[1]['question']}\n   Choices:\n   {qa_pairs[1]['choices']}\n   Answer: {qa_pairs[1]['answer']}\n   Reasoning: {qa_pairs[1]['reasoning']}\n\n3. Question: {qa_pairs[2]['question']}\n   Choices:\n   {qa_pairs[2]['choices']}\n   Answer: {qa_pairs[2]['answer']}\n   Reasoning: {qa_pairs[1]['reasoning']}\n\nNow, please answer the following question:\n<|im_end|>\n<|im_start|>user\nQuestion: {question}\nChoices:\n{formatted_choices}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\nimport re\n\ndef extract_number_after_assistant(text):\n    # Search for the number after \"assistant\" or \"ANSWER:\"\n    match = re.search(r\"assistant\\s*(?:ANSWER:\\s*)?(\\d+)\", text)\n    if match:\n        return int(match.group(1))  # Extract and return the number\n    else:\n        return None  # Return None if no match is found\n\ndef index_to_letter(index):\n    try:\n        letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n        letter = letters[index]\n    except:\n        letter = 'A'\n    return letter\n\ndef get_index_answer(prompt, max_new_tokens=5):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        top_p=1.0,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    if \"<|im_start|>assistant\" in decoded:\n        result = decoded.split(\"<|im_start|>assistant\")[-1].strip()\n    else:\n        result = decoded.strip()\n    # print(result)\n    # print(\"--------------------------------\")\n    # print(extract_index_from_output(result))\n    # print(extract_number_after_assistant(result))\n    return index_to_letter((extract_index_from_output(result)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.951974Z","iopub.status.idle":"2025-04-23T09:21:35.952385Z","shell.execute_reply":"2025-04-23T09:21:35.952213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def index_to_letter(index, zero_based=True):\n    \"\"\"\n    Convert an index to a corresponding letter (A, B, C, ...).\n\n    Args:\n        index (int): The index to convert.\n        zero_based (bool): If True, treats the index as zero-based (0 -> A).\n                           If False, treats the index as one-based (1 -> A).\n\n    Returns:\n        str: The corresponding letter.\n    \"\"\"\n    if not zero_based:\n        index -= 1  # Convert 1-based index to 0-based\n    return chr(65 + index)  # 65 is the ASCII value of 'A'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.953607Z","iopub.status.idle":"2025-04-23T09:21:35.954014Z","shell.execute_reply":"2025-04-23T09:21:35.953838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = []\n\nfrom tqdm import tqdm\n\nfor idx, row in tqdm(questions_df.iterrows(), total=len(questions_df)):\n    task_id = row['task_id']\n    question = row['question']\n    # print(question)\n    choices_str = row['choices']\n\n    qa_pairs = retrieval(idx, csv_data_path, train_pkl_path, test_pkl_path, k=3)\n\n    prompt = format_prompt(question, choices_str, qa_pairs)\n\n    index = get_index_answer(prompt)\n    preds.append(index)\n\n\nsubmission = pd.DataFrame({\n    \"task_id\": test_df[\"task_id\"],\n    \"answer\": preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Đã tạo submission.csv xong.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.955044Z","iopub.status.idle":"2025-04-23T09:21:35.955437Z","shell.execute_reply":"2025-04-23T09:21:35.955272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"task_id\": questions_df[\"task_id\"],\n    \"answer\": preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Đã tạo submission.csv xong.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.956310Z","iopub.status.idle":"2025-04-23T09:21:35.956678Z","shell.execute_reply":"2025-04-23T09:21:35.956514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef batch_infer(questions_df, csv_data_path, train_pkl_path, test_pkl_path, batch_size=4):\n    preds = []\n\n    # Process in batches\n    for start_idx in tqdm(range(0, len(questions_df), batch_size), total=(len(questions_df) // batch_size) + 1):\n        end_idx = min(start_idx + batch_size, len(questions_df))\n        batch = questions_df.iloc[start_idx:end_idx]\n\n        for idx, row in batch.iterrows():\n            task_id = row['task_id']\n            question = row['question']\n            choices_str = row['choices']\n\n            # Retrieve relevant QA pairs\n            qa_pairs = retrieval(idx, csv_data_path, train_pkl_path, test_pkl_path, k=3)\n\n            # Format the prompt\n            prompt = format_prompt(question, choices_str, qa_pairs)\n\n            # Get the predicted index\n            index = get_index_answer(prompt)\n            preds.append(index)\n\n    return preds\n\n\n\n# Example usage\npreds = batch_infer(questions_df, csv_data_path, train_pkl_path, test_pkl_path, batch_size=32)\n\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    \"task_id\": questions_df[\"task_id\"],\n    \"answer\": preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Đã tạo submission.csv xong.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T09:21:35.957252Z","iopub.status.idle":"2025-04-23T09:21:35.957562Z","shell.execute_reply":"2025-04-23T09:21:35.957446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Batch Processing","metadata":{}}]}